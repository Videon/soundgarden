In this entry, I will focus on the ideas behind the sound implementation in Soundgarden, my approach in adapting Schafer's Soniferous Garden and acoustic design principles.

**From reality to simulated reality by applying observed patterns** üîé

The figure below depicts the audible activity of three different animal groups over a 12-hour period. The curves which are derived from observed animal activity resemble curves that game engines use to set continuous values.
![](attachments/Pasted%20image%2020240320202008.png)
From: Schafer, The Soundscape, p. 231

We could now na√Øvely copypaste these curves into the game engine and have them control the activity of actors directly. However, to adapt a real-world phenomenon, we must understand the relationships that led to its emergence. Specifically looking at the bird activity, there is activity starting at approximately 3 AM and ending at around 10 PM, which appears to correlate with sunrise and sunset. This leads to the conclusion that it is not the time of day that affects activity here (of course, birds don't wear watches, duh) but rather brightness, affected by the increasing and decreasing intensity of sunlight throughout the day. The introduction of this sun intensity as a systemic factor may in turn inspire more thinking about other entities in the game world that could be affected by variations in sun intensity.


Working with game engines involves an inversion of the process that yields data from observations. Here, by setting the data either directly (e.g., drawing an animation curve that changes values over time), or modifying procedural actors, which leads to an emergence of data (e.g., imagine having procedurally generated flocks of birds that are programmed to be active at certain times of day) has a significant influence on the resulting data.

- Add day/night cycle using these curves


**Situating the island sounds**

I created sound objects that would be symbols for certain areas on the map.
Schafer differentiates between Keynotes, Signals, and Soundmarks. Keynotes are those sounds that are not necessarily listened to consciously but are inherent to an environment or setting, providing a background sound against which other sounds are perceived. They are often likened to a musical keynote, which provides the tonal foundation of a musical piece but might not be immediately noticeable. In a natural environment, keynotes could be the sound of wind in the trees or water in a stream; in an urban environment, it might be the hum of traffic or air conditioning systems. These sounds define the character of the place and are often heard by those living within the environment as the sound of home or normality.

Keynotes
- Waves clashing in the background, washing over the shore

Signals
- Birds flying by ???

Soundmarks
- Cove: Musical frogs
- Hill: Musical wind
- Beach: Musical waves

Soundmarks are the unique sounds that define a location. Since the island environment is not based on a real place, one might argue that there cannot be any soundmarks here.
However, what defines this island are its musical sounds, which are unique to this virtual locus. I will therefore interpret these as soundmarks.


- Keep in mind new environmental sounds need custom attenuation curves

Constantly playing background music seems to closely resemble the act of moving through the world with headphones in your ears, possibly shutting off the world around you. Soundgarden is about listening to the interplay of sound and silence, and gathering meaning from it. To address this, and in light of the approaching deadline for this project at the end of this month, I have decided to introduce one last major change to Soundgarden - integrating the extradiegetic music into the game environment.


**Modular structures**



**On Normalization** üéöÔ∏è

Whenever a logical component, for example a module in my music system, uses float parameters for input or output, I map them to a range between 0 and 1, also referred to as a unit interval in mathematics. Not only allows this for easier connection of different components because values don't need to be remapped, it allows for "logical compression", i.e., deriving meaning from a value and its name. 0 will always be the least of something, or off, whereas 1 will always be the maximum, the most intense. See the section on parrots above for another example.

If all parts of the music system, as well as game logic, use normalized values as in/outputs, connecting the individual components becomes trivial.

There is also a simple implementation of music intensity that goes up the further the player is away from the next stone. Should play around with distances more here. Currently, intensity (0...1) is connected to some rhythmic elements, leading to more intense percussion patterns (=more hits) with values towards intensity = 1 when the player is far away from the goal, and no percussion at all (intensity = 0) when the player is close to the goal. The idea is to animate the player to move around and look for the stone using more animated percussive patterns.

They also help us in understanding relations



I reduced the island size. While the feeling of wandering can be a mode of experience, it felt too tiresome to walk all the way around the island looking for a collectible. 



I think what all of these songs have in common is that they feature short, repetitive musical structures. These structures lend themselves well for algorithmic recreation. Combining a series of modular music generators, each in itself a rather simple design, leads to a  Due to their concise nature and combination of a minimal amount of rhythmical and melodic elements, with a nevertheless complex and, perhaps even pleasant nature that 