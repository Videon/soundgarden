In this entry, I will focus on the ideas behind the sound implementation in Soundgarden and outline my approach in adapting Schafer's Soniferous Garden and acoustic design principles, explaining some of the reasoning behind the sounds and systems that I've been silently adding over the previous weeks.


**Principles of acoustic design** üîä
Schafer mentions four principles of acoustic design (The Soundscape, p. 238), which essentially boil down to the following terms that I will explain in the context of my own work with Soundgarden.

- **Composition:** Sounds in a game environment should be chosen and arranged such that they contribute to an immersive and harmonious soundscape, while respecting the player's auditory threshold, i.e., avoiding harmfully loud or imperceptibly quiet sounds.
- **Meaning:** Sounds should be more than mere functional cues. A sound may already hold symbolic depth or may be imbued with it when integrated into a game environment. This may allow for additional emotional layers, and enhancement of existing narratives. Sounds may resonate with players when using auditory symbols, evoking memories, emotions, and personal connections.
- **Timing:** Sounds may occur at different points in time, constantly overlapping or alternating with each other. A sound may have its own rhythm and tempo, and/or may be controlled by the rhythms of other actors and systems in a game. 
- **Self-regulation:** Sounds should be capable of moderating themselves to prevent auditory overload. This is especially important when a sound is driven procedurally, where a range of  outcomes, including extreme ones, is possible.


**From reality to simulated reality by applying observed patterns** üîé

The figure below depicts the audible activity of three different animal groups over a 12-hour period. The curves which are derived from observed animal activity resemble curves that game engines use to set continuous values.
![](attachments/Pasted%20image%2020240320202008.png)
From: Schafer, The Soundscape, p. 231

We could now na√Øvely copypaste these curves into the game engine and have them control the activity of actors directly. However, to adapt a real-world phenomenon, we must understand the relationships that led to its emergence. Specifically looking at the bird activity, there is activity starting at approximately 3 AM and ending at around 10 PM, which appears to correlate with sunrise and sunset. This leads to the conclusion that it is not the time of day that affects activity here (of course, birds don't wear watches, duh) but rather brightness, affected by the increasing and decreasing intensity of sunlight throughout the day. The introduction of this sun intensity as a systemic factor may in turn inspire more thinking about other entities in the game world that could be affected by variations in sun intensity.


Working with game engines involves an inversion of the process that yields data from observations. Here, by setting the data either directly (e.g., drawing an animation curve that changes values over time), or modifying procedural actors, which leads to an emergence of data (e.g., imagine having procedurally generated flocks of birds that are programmed to be active at certain times of day) has a significant influence on the resulting data.

- Add day/night cycle using these curves


**Situating the island sounds**

I created sound objects that would be symbols for certain areas on the map.
Schafer differentiates between Keynotes, Signals, and Soundmarks. Keynotes are those sounds that are not necessarily listened to consciously but are inherent to an environment or setting, providing a background sound against which other sounds are perceived. They are often likened to a musical keynote, which provides the tonal foundation of a musical piece but might not be immediately noticeable. In a natural environment, keynotes could be the sound of wind in the trees or water in a stream; in an urban environment, it might be the hum of traffic or air conditioning systems. These sounds define the character of the place and are often heard by those living within the environment as the sound of home or normality.

Keynotes
- Waves clashing in the background, washing over the shore

Signals
- Birds flying by ???

Soundmarks
- Cove: Musical frogs
- Hill: Musical wind
- Beach: Musical waves

Soundmarks are the unique sounds that define a location. Since the island environment is not based on a real place, one might argue that there cannot be any soundmarks here.
However, what defines this island are its musical sounds, which are unique to this virtual locus. I will therefore interpret these as soundmarks.


- Keep in mind new environmental sounds need custom attenuation curves

Constantly playing background music seems to closely resemble the act of moving through the world with headphones in your ears, possibly shutting off the world around you. Soundgarden is about listening to the interplay of sound and silence, and gathering meaning from it. To address this, and in light of the approaching deadline for this project at the end of this month, I have decided to introduce one last major change to Soundgarden - integrating the extradiegetic music into the game environment.


**Modular structures**



**On Normalization** üéöÔ∏è

Whenever a logical component, for example a module in my music system, uses float parameters for input or output, I map them to a range between 0 and 1, also referred to as a unit interval in mathematics. Not only allows this for easier connection of different components because values don't need to be remapped, it allows for "logical compression", i.e., deriving meaning from a value and its name. 0 will always be the least of something, or off, whereas 1 will always be the maximum, the most intense. See the section on parrots above for another example.

If all parts of the music system, as well as game logic, use normalized values as in/outputs, connecting the individual components becomes trivial.

There is also a simple implementation of music intensity that goes up the further the player is away from the next stone. Should play around with distances more here. Currently, intensity (0...1) is connected to some rhythmic elements, leading to more intense percussion patterns (=more hits) with values towards intensity = 1 when the player is far away from the goal, and no percussion at all (intensity = 0) when the player is close to the goal. The idea is to animate the player to move around and look for the stone using more animated percussive patterns.

They also help us in understanding relations



I reduced the island size. While the feeling of wandering can be a mode of experience, it felt too tiresome to walk all the way around the island looking for a collectible. 



I think what all of these songs have in common is that they feature short, repetitive musical structures. These structures lend themselves well for algorithmic recreation. Combining a series of modular music generators, each in itself a rather simple design, leads to a  Due to their concise nature and combination of a minimal amount of rhythmical and melodic elements, with a nevertheless complex and, perhaps even pleasant nature that 

**Filling in the gaps**




I first wanted to have the music change according to the sun intensity, to essentially have different musical settings based on whether it's day or nighttime. However, after moving all the music to the diegetic environment space, things fell into place: Since sun intensity already dictates the audible activities of certain elements, that already lead to varying music at different times of day - another example of systemic thinking's beneficial impacts on game design!



04-04-2024 8:00 PM
I found something. The intuition. And with it, an acceptance of where this journey is going.